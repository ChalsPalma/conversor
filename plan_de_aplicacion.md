# Plan de Aplicaci√≥n: Conversi√≥n de PDF a Embeddings Vectoriales

## 1. Objetivo y Funcionalidad Principal

El prop√≥sito fundamental de esta aplicaci√≥n es transformar documentos PDF no estructurados en una representaci√≥n estructurada y sem√°nticamente rica, utilizando embeddings vectoriales. Esta herramienta permitir√° a los usuarios extraer el conocimiento contenido en sus archivos PDF de una manera que sea f√°cilmente consultable y analizable por sistemas de inteligencia artificial.

### Funci√≥n Principal

La funci√≥n principal de la aplicaci√≥n es leer archivos PDF, procesar su contenido textual y generar embeddings vectoriales para cada "trozo" o "chunk" de texto significativo. Este proceso implica varias etapas, desde la extracci√≥n robusta del texto hasta la segmentaci√≥n inteligente y la vectorizaci√≥n mediante modelos de lenguaje avanzados.

### Casos de Uso

Los embeddings generados por esta aplicaci√≥n tienen una amplia gama de aplicaciones pr√°cticas, incluyendo:

*   **Bases de Datos Vectoriales para B√∫squeda Sem√°ntica:** Los embeddings se almacenar√°n en bases de datos vectoriales, lo que permitir√° realizar b√∫squedas basadas en el significado contextual en lugar de solo palabras clave. Esto es crucial para sistemas de recuperaci√≥n de informaci√≥n m√°s inteligentes y precisos.
*   **Asistentes de IA y Chatbots:** Los embeddings pueden alimentar asistentes virtuales o chatbots que necesitan "comprender" el contenido de documentos extensos para responder preguntas, resumir informaci√≥n o proporcionar insights relevantes. Esto es particularmente √∫til en dominios como el legal, m√©dico o de soporte al cliente.
*   **An√°lisis de Documentos y Clasificaci√≥n:** Al representar documentos como vectores, se pueden aplicar t√©cnicas de aprendizaje autom√°tico para clasificar, agrupar o analizar grandes colecciones de documentos de manera eficiente.
*   **Generaci√≥n de Res√∫menes y Extracci√≥n de Informaci√≥n:** Los embeddings facilitan la identificaci√≥n de las partes m√°s relevantes de un documento, lo que puede ser utilizado para generar res√∫menes autom√°ticos o extraer entidades y relaciones clave.

## 2. Requisitos T√©cnicos y Tecnolog√≠as

Para construir una aplicaci√≥n robusta, eficiente y escalable, se han seleccionado las siguientes tecnolog√≠as, priorizando el ecosistema de Python debido a su madurez y la riqueza de sus librer√≠as para el procesamiento de lenguaje natural y la inteligencia artificial.

### Lenguaje de Programaci√≥n

*   **Python:** Es el lenguaje elegido por su versatilidad, su vasta colecci√≥n de librer√≠as para IA y ML, y su facilidad de integraci√≥n con diversas herramientas y plataformas. Su sintaxis clara y su gran comunidad de desarrolladores lo hacen ideal para este tipo de proyectos.

### Librer√≠as Clave

La selecci√≥n de librer√≠as se basa en la necesidad de cubrir cada etapa del pipeline de procesamiento de PDF a embeddings:

*   **Extracci√≥n de Texto de PDF:**
    *   **PyMuPDF (Fitz):** Conocido por su velocidad y eficiencia en la extracci√≥n de texto, im√°genes y metadatos de archivos PDF. Es una opci√≥n robusta para manejar una amplia variedad de formatos de PDF [1].
    *   **pdfplumber:** Ofrece una API m√°s amigable para la extracci√≥n de texto y datos tabulares, lo que puede ser ventajoso para PDFs con estructuras complejas [2]. Se considerar√° como alternativa o complemento a PyMuPDF.

*   **Divisi√≥n de Texto en "Chunks" y Gesti√≥n del Flujo de Trabajo:**
    *   **LangChain:** Un framework potente para el desarrollo de aplicaciones impulsadas por modelos de lenguaje. Proporciona herramientas para el "chunking" de texto, la gesti√≥n de cadenas de procesamiento (chains) y la integraci√≥n con bases de datos vectoriales y modelos de embeddings [3].
    *   **LlamaIndex:** Similar a LangChain, se especializa en la construcci√≥n de aplicaciones con LLMs sobre datos personalizados. Ofrece excelentes capacidades para la indexaci√≥n y recuperaci√≥n de informaci√≥n, lo que lo hace ideal para la gesti√≥n de documentos y la creaci√≥n de √≠ndices vectoriales [4]. Se utilizar√° LangChain por su popularidad y amplia adopci√≥n, pero LlamaIndex es una alternativa viable.

*   **Generaci√≥n de Embeddings:**
    *   **Sentence-Transformers:** Una librer√≠a para generar embeddings de oraciones, p√°rrafos e im√°genes. Proporciona una gran cantidad de modelos pre-entrenados que son eficientes y de alta calidad para diversas tareas de PNL [5].
    *   **API de OpenAI:** Para embeddings de mayor calidad y escalabilidad, se puede integrar la API de embeddings de OpenAI (por ejemplo, `text-embedding-ada-002`). Esto ofrece un rendimiento superior a costa de un costo por uso [6]. La elecci√≥n depender√° de los requisitos de calidad y presupuesto.

*   **Base de Datos Vectorial:**
    *   **ChromaDB:** Una base de datos vectorial de c√≥digo abierto, ligera y f√°cil de usar, ideal para prototipos y aplicaciones de menor escala. Permite almacenar embeddings y sus metadatos, y realizar b√∫squedas de similitud [7].
    *   **Pinecone:** Una base de datos vectorial gestionada en la nube, dise√±ada para la escalabilidad y el rendimiento en producci√≥n. Es adecuada para grandes vol√∫menes de datos y altas tasas de consulta [8]. Se optar√° por ChromaDB para la implementaci√≥n inicial debido a su facilidad de configuraci√≥n local.

*   **Interfaz de Usuario (UI):**
    *   **Streamlit:** Un framework de c√≥digo abierto que permite crear aplicaciones web interactivas para ciencia de datos y aprendizaje autom√°tico con solo unas pocas l√≠neas de c√≥digo Python. Es ideal para construir una UI simple y funcional r√°pidamente [9].
    *   **Gradio:** Similar a Streamlit, facilita la creaci√≥n de interfaces de usuario para modelos de ML. Es otra opci√≥n viable para la UI [10]. Se utilizar√° Streamlit por su popularidad y flexibilidad.

## 3. Arquitectura de la Aplicaci√≥n

La aplicaci√≥n seguir√° una arquitectura modular, dividiendo el proceso en componentes l√≥gicos que interact√∫an entre s√≠. Esto facilita el desarrollo, la depuraci√≥n y la escalabilidad.

```mermaid
graph TD
    A[Usuario] -->|Sube PDF| B(Frontend: Streamlit)
    B -->|Env√≠a PDF| C(Backend: L√≥gica de Procesamiento)
    C -->|Paso 1: Carga del PDF| D{M√≥dulo de Carga}
    D -->|Paso 2: Extracci√≥n de Texto| E{M√≥dulo de Extracci√≥n: PyMuPDF}
    E -->|Paso 3: Chunking| F{M√≥dulo de Chunking: LangChain}
    F -->|Paso 4: Generaci√≥n de Embeddings| G{M√≥dulo de Embeddings: Sentence-Transformers/OpenAI API}
    G -->|Paso 5: Almacenamiento| H[Base de Datos Vectorial: ChromaDB]
    H -->|Almacena Embeddings y Metadatos| I(Almacenamiento Persistente)
```

### Frontend (UI)

La interfaz de usuario ser√° desarrollada con Streamlit. Su funci√≥n principal es proporcionar un punto de entrada intuitivo para el usuario:

*   **Carga de Archivos:** Un componente de carga de archivos que permita al usuario seleccionar y subir uno o varios archivos PDF.
*   **Feedback Visual:** Mostrar el estado del procesamiento (cargando, extrayendo, generando embeddings, completado) y cualquier mensaje de error.
*   **Configuraci√≥n (Opcional):** Posibilidad de configurar par√°metros como el tama√±o del chunk o el modelo de embeddings (para usuarios avanzados).

### Backend (L√≥gica de Procesamiento)

El backend es el coraz√≥n de la aplicaci√≥n, donde se ejecuta toda la l√≥gica de procesamiento. Se compone de los siguientes pasos:

#### Paso 1: Carga del PDF

*   **Funci√≥n:** Recibir el archivo PDF subido desde el frontend. Esto implica manejar la entrada de archivos y guardarlos temporalmente en el sistema de archivos o en memoria para su posterior procesamiento.
*   **Tecnolog√≠a:** Manejado por Streamlit en el lado del servidor, que proporciona objetos de archivo para el contenido subido.

#### Paso 2: Extracci√≥n de Texto

*   **Funci√≥n:** Extraer todo el texto legible del PDF. Este es un paso cr√≠tico que debe manejar diversas complejidades de los PDFs, como texto en diferentes fuentes, columnas, tablas, y la presencia de im√°genes.
*   **Tecnolog√≠a:** **PyMuPDF** ser√° la librer√≠a principal. Se encargar√° de iterar sobre las p√°ginas del PDF y extraer el texto, intentando preservar la estructura en la medida de lo posible. Se implementar√°n mecanismos para manejar tablas (si es posible, extraerlas como texto estructurado) y saltos de p√°gina.

#### Paso 3: Chunking (Divisi√≥n de Texto)

*   **Funci√≥n:** Dividir el texto extra√≠do en fragmentos m√°s peque√±os y manejables. El objetivo es crear "chunks" que sean lo suficientemente peque√±os para ser procesados eficientemente por los modelos de embeddings, pero lo suficientemente grandes para mantener el contexto sem√°ntico.
*   **Tecnolog√≠a:** **LangChain** proporcionar√° herramientas como `RecursiveCharacterTextSplitter`. Este splitter es capaz de dividir el texto bas√°ndose en una lista de separadores de caracteres, lo que permite un control fino sobre c√≥mo se dividen los documentos (por ejemplo, por p√°rrafos, oraciones, o caracteres). Se definir√° un tama√±o de chunk (ej., 500 caracteres) y un solapamiento (ej., 100 caracteres) para asegurar la continuidad del contexto entre chunks adyacentes.

#### Paso 4: Generaci√≥n de Embeddings

*   **Funci√≥n:** Convertir cada fragmento de texto en un vector num√©rico (embedding). Este vector captura el significado sem√°ntico del texto, permitiendo comparaciones de similitud.
*   **Tecnolog√≠a:** Se utilizar√° **Sentence-Transformers** para la generaci√≥n de embeddings localmente. Esto evita dependencias externas y costos de API. Para casos de uso que requieran mayor precisi√≥n o modelos m√°s avanzados, se puede integrar la **API de OpenAI** como una opci√≥n configurable.

#### Paso 5: Almacenamiento

*   **Funci√≥n:** Almacenar los pares (texto original del chunk, embedding vectorial) en una base de datos vectorial. Adem√°s de los embeddings, se almacenar√°n metadatos relevantes para cada chunk.
*   **Tecnolog√≠a:** **ChromaDB** ser√° la base de datos vectorial utilizada. Es una soluci√≥n ligera y f√°cil de integrar que permite almacenar colecciones de embeddings y realizar b√∫squedas de similitud de manera eficiente. Se crear√° una colecci√≥n espec√≠fica para cada documento o para un conjunto de documentos relacionados.

### Base de Datos

*   **Tipo:** Base de datos vectorial (ChromaDB).
*   **Contenido:**
    *   **Vectores (Embeddings):** La representaci√≥n num√©rica de cada chunk de texto.
    *   **Texto Original:** El fragmento de texto correspondiente a cada embedding, para referencia y visualizaci√≥n.
    *   **Metadatos:** Informaci√≥n adicional crucial para la recuperaci√≥n y el filtrado, como:
        *   `nombre_archivo`: Nombre del archivo PDF de origen.
        *   `numero_pagina`: N√∫mero de p√°gina dentro del PDF donde se encuentra el chunk.
        *   `offset_inicio`: Posici√≥n de inicio del chunk dentro del texto de la p√°gina (opcional, para depuraci√≥n).
        *   `offset_fin`: Posici√≥n de fin del chunk dentro del texto de la p√°gina (opcional, para depuraci√≥n).
        *   `id_chunk`: Un identificador √∫nico para cada chunk.

Esta arquitectura modular asegura que cada componente pueda ser desarrollado, probado y optimizado de forma independiente, facilitando el mantenimiento y la evoluci√≥n de la aplicaci√≥n.




![Diagrama de Arquitectura](arquitectura_aplicacion.png)





## Referencias

[1] PyMuPDF Documentation: [https://pymupdf.readthedocs.io/en/latest/](https://pymupdf.readthedocs.io/en/latest/)
[2] pdfplumber Documentation: [https://github.com/jsvine/pdfplumber](https://github.com/jsvine/pdfplumber)
[3] LangChain Documentation: [https://python.langchain.com/docs/](https://python.langchain.com/docs/)
[4] LlamaIndex Documentation: [https://docs.llamaindex.ai/en/stable/](https://docs.llamaindex.ai/en/stable/)
[5] Sentence-Transformers Documentation: [https://www.sbert.net/](https://www.sbert.net/)
[6] OpenAI Embeddings API: [https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings)
[7] ChromaDB Documentation: [https://www.trychroma.com/](https://www.trychroma.com/)
[8] Pinecone Documentation: [https://www.pinecone.io/docs/](https://www.pinecone.io/docs/)
[9] Streamlit Documentation: [https://streamlit.io/](https://streamlit.io/)
[10] Gradio Documentation: [https://gradio.app/](https://gradio.app/)





## 4. Solicitud de C√≥digo y Fragmentos Espec√≠ficos

Esta secci√≥n detalla los fragmentos de c√≥digo esenciales para cada componente de la aplicaci√≥n, proporcionando una gu√≠a pr√°ctica para su implementaci√≥n.

### Fragmento 1: Extracci√≥n de Texto con PyMuPDF

La extracci√≥n de texto de documentos PDF es el primer paso cr√≠tico en el pipeline. PyMuPDF (tambi√©n conocido como `fitz`) es una librer√≠a de Python que destaca por su velocidad y eficiencia en esta tarea. Permite acceder al contenido textual de cada p√°gina de un PDF, manejar diferentes codificaciones y, en cierta medida, preservar la estructura del documento.

La funci√≥n `extract_text_from_pdf` toma la ruta de un archivo PDF como entrada y devuelve una √∫nica cadena de texto que contiene todo el contenido extra√≠do del documento. Se incluye un manejo b√°sico de errores para asegurar que solo se procesen archivos PDF v√°lidos y para capturar posibles problemas durante la extracci√≥n.

```python
import fitz  # PyMuPDF

def extract_text_from_pdf(pdf_path: str) -> str:
    """
    Extrae todo el texto de un archivo PDF.

    Args:
        pdf_path (str): La ruta al archivo PDF.

    Returns:
        str: Una cadena de texto que contiene todo el contenido del PDF.
             Retorna una cadena vac√≠a si el archivo no es un PDF o est√° da√±ado.
    """
    text = ""
    try:
        with fitz.open(pdf_path) as doc:
            for page_num in range(doc.page_count):
                page = doc.load_page(page_num)
                text += page.get_text()
    except fitz.FileDataError:
        print(f"Error: El archivo {pdf_path} no es un PDF v√°lido o est√° da√±ado.")
        return ""
    except Exception as e:
        print(f"Ocurri√≥ un error inesperado al procesar {pdf_path}: {e}")
        return ""
    return text

# Ejemplo de uso (para pruebas locales)
if __name__ == "__main__":
    # Crea un PDF de ejemplo para probar la funci√≥n
    # En un entorno real, usar√≠as un archivo PDF existente
    try:
        doc = fitz.open()
        page = doc.new_page()
        page.insert_text((72, 72), "Este es un documento PDF de prueba.\n")
        page.insert_text((72, 100), "Contiene varias l√≠neas de texto para demostraci√≥n.\n")
        page.insert_text((72, 128), "La extracci√≥n de texto es el primer paso crucial.")
        doc.save("ejemplo.pdf")
        doc.close()

        extracted_content = extract_text_from_pdf("ejemplo.pdf")
        print("\n--- Contenido Extra√≠do ---")
        print(extracted_content)

        # Prueba con un archivo no existente
        print("\n--- Prueba con archivo no existente ---")
        extract_text_from_pdf("no_existe.pdf")

        # Limpiar el archivo de ejemplo
        import os
        os.remove("ejemplo.pdf")
    except ImportError:
        print("PyMuPDF no est√° instalado. Por favor, inst√°lalo con: pip install PyMuPDF")
    except Exception as e:
        print(f"Error durante la ejecuci√≥n del ejemplo: {e}")

```

**Explicaci√≥n del C√≥digo:**

*   **`import fitz`**: Importa la librer√≠a PyMuPDF.
*   **`fitz.open(pdf_path)`**: Abre el archivo PDF especificado por `pdf_path`. Se utiliza un bloque `with` para asegurar que el documento se cierre correctamente.
*   **`doc.page_count`**: Obtiene el n√∫mero total de p√°ginas en el documento.
*   **`doc.load_page(page_num)`**: Carga una p√°gina espec√≠fica del PDF.
*   **`page.get_text()`**: Extrae el texto de la p√°gina actual. Este m√©todo intenta preservar el formato y el orden del texto lo mejor posible.
*   **Manejo de Errores**: Los bloques `try-except` capturan `fitz.FileDataError` para archivos PDF inv√°lidos o da√±ados, y un `Exception` general para otros errores inesperados. Esto mejora la robustez de la funci√≥n.
*   **Ejemplo de Uso (`if __name__ == "__main__"`)**: Se incluye un bloque de c√≥digo que demuestra c√≥mo crear un PDF simple, extraer su texto y manejar un caso de error (archivo no existente). Este bloque es √∫til para probar la funci√≥n de forma aislada.

Esta funci√≥n proporciona una base s√≥lida para la extracci√≥n de texto, que es fundamental para los pasos posteriores de chunking y generaci√≥n de embeddings.




### Fragmento 2: Chunking con LangChain

Una vez que el texto ha sido extra√≠do del PDF, el siguiente paso es dividirlo en "chunks" o fragmentos. Este proceso es crucial porque los modelos de embeddings tienen limitaciones en la cantidad de texto que pueden procesar a la vez. Adem√°s, dividir el texto de manera inteligente ayuda a preservar el contexto sem√°ntico dentro de cada fragmento, lo que es vital para la calidad de los embeddings y la posterior recuperaci√≥n de informaci√≥n.

LangChain ofrece varias estrategias para el "chunking" de texto, y `RecursiveCharacterTextSplitter` es una de las m√°s flexibles y recomendadas. Este splitter intenta dividir el texto utilizando una lista de separadores de caracteres (por defecto, `["\n\n", "\n", " ", ""]`). Si un chunk es demasiado grande, intenta dividirlo con el siguiente separador en la lista, y as√≠ sucesivamente. Esto permite una divisi√≥n m√°s natural y contextual del texto.

La funci√≥n `split_text_into_chunks` toma una cadena de texto, un tama√±o de chunk (`chunk_size`) y un tama√±o de solapamiento (`chunk_overlap`) como entrada, y devuelve una lista de cadenas de texto (los chunks).

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

def split_text_into_chunks(text: str, chunk_size: int = 500, chunk_overlap: int = 100) -> list[str]:
    """
    Divide una cadena de texto en chunks utilizando RecursiveCharacterTextSplitter de LangChain.

    Args:
        text (str): La cadena de texto a dividir.
        chunk_size (int): El tama√±o m√°ximo de cada chunk.
        chunk_overlap (int): El n√∫mero de caracteres que se solapan entre chunks adyacentes.

    Returns:
        list[str]: Una lista de cadenas de texto (chunks).
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,  # Usa la longitud de caracteres como m√©trica
        add_start_index=True, # A√±ade el √≠ndice de inicio de cada chunk
    )
    chunks = text_splitter.split_text(text)
    return chunks

# Ejemplo de uso (para pruebas locales)
if __name__ == "__main__":
    sample_text = (
        "El procesamiento del lenguaje natural (PLN) es un campo de la inteligencia artificial que se ocupa de la interacci√≥n entre computadoras y el lenguaje humano. "
        "Uno de los desaf√≠os clave en el PLN es la comprensi√≥n del contexto y el significado de las palabras. "
        "Las t√©cnicas de chunking son fundamentales para preparar grandes vol√∫menes de texto para su an√°lisis. "
        "Permiten dividir el texto en unidades m√°s peque√±as que son m√°s manejables para los modelos de aprendizaje autom√°tico. "
        "El solapamiento entre chunks ayuda a mantener la continuidad sem√°ntica y a evitar la p√©rdida de informaci√≥n importante en los l√≠mites de los chunks. "
        "Esto es especialmente relevante cuando se trabaja con documentos extensos como libros o informes t√©cnicos. "
        "La elecci√≥n del tama√±o del chunk y el solapamiento depende de la aplicaci√≥n espec√≠fica y del modelo de embedding utilizado."
    )

    print("\n--- Texto Original ---")
    print(sample_text)

    print("\n--- Chunks Generados (tama√±o=100, solapamiento=20) ---")
    chunks = split_text_into_chunks(sample_text, chunk_size=100, chunk_overlap=20)
    for i, chunk in enumerate(chunks):
        print(f"Chunk {i+1} (longitud: {len(chunk)}):\n{chunk}\n")

    print("\n--- Chunks Generados (tama√±o=50, solapamiento=10) ---")
    chunks_small = split_text_into_chunks(sample_text, chunk_size=50, chunk_overlap=10)
    for i, chunk in enumerate(chunks_small):
        print(f"Chunk {i+1} (longitud: {len(chunk)}):\n{chunk}\n")
```

**Explicaci√≥n del C√≥digo:**

*   **`from langchain.text_splitter import RecursiveCharacterTextSplitter`**: Importa la clase necesaria de LangChain.
*   **`RecursiveCharacterTextSplitter(...)`**: Se inicializa el splitter con los par√°metros `chunk_size` (tama√±o m√°ximo de cada fragmento) y `chunk_overlap` (cantidad de caracteres que se solapan entre fragmentos adyacentes). El `length_function=len` indica que la longitud se mide en caracteres. `add_start_index=True` es √∫til para depuraci√≥n y para asociar los chunks con su posici√≥n original en el documento.
*   **`text_splitter.split_text(text)`**: Este m√©todo realiza la divisi√≥n del texto seg√∫n la configuraci√≥n del splitter.
*   **Ejemplo de Uso (`if __name__ == "__main__"`)**: Se proporciona un texto de ejemplo y se demuestra c√≥mo dividirlo en chunks con diferentes configuraciones de `chunk_size` y `chunk_overlap`. Esto permite visualizar el efecto de estos par√°metros en la salida.

La implementaci√≥n de esta funci√≥n de chunking es un paso fundamental para preparar el texto extra√≠do del PDF para la fase de generaci√≥n de embeddings, asegurando que los fragmentos sean de un tama√±o adecuado y mantengan el contexto sem√°ntico.




### Fragmento 3: Generaci√≥n de Embeddings y Almacenamiento en ChromaDB

La generaci√≥n de embeddings es el proceso de transformar el texto en representaciones num√©ricas densas que capturan su significado sem√°ntico. Estos vectores son la base para realizar b√∫squedas de similitud y otras operaciones de PNL. Posteriormente, estos embeddings se almacenan en una base de datos vectorial, como ChromaDB, que est√° optimizada para almacenar y consultar vectores de manera eficiente.

La funci√≥n `generate_embeddings_and_store` integra los pasos anteriores (extracci√≥n y chunking) con la generaci√≥n de embeddings y el almacenamiento. Utiliza `SentenceTransformer` para generar los embeddings localmente y `ChromaDB` para la persistencia de los datos.

```python
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.utils import embedding_functions
import os

# Aseg√∫rate de que las funciones de extracci√≥n y chunking est√©n disponibles
# from pdf_extractor import extract_text_from_pdf
# from text_chunker import split_text_into_chunks

def generate_embeddings_and_store(
    pdf_path: str,
    collection_name: str = "pdf_embeddings",
    model_name: str = "all-MiniLM-L6-v2",
    chunk_size: int = 500,
    chunk_overlap: int = 100
):
    """
    Extrae texto de un PDF, lo divide en chunks, genera embeddings y los almacena en ChromaDB.

    Args:
        pdf_path (str): La ruta al archivo PDF.
        collection_name (str): El nombre de la colecci√≥n en ChromaDB donde se almacenar√°n los embeddings.
        model_name (str): El nombre del modelo de Sentence-Transformers a utilizar.
        chunk_size (int): El tama√±o de los chunks de texto.
        chunk_overlap (int): El solapamiento entre chunks de texto.
    """
    print(f"Procesando PDF: {pdf_path}")
    text = extract_text_from_pdf(pdf_path)
    if not text:
        print(f"No se pudo extraer texto de {pdf_path}. Abortando.")
        return

    chunks = split_text_into_chunks(text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    if not chunks:
        print(f"No se generaron chunks para {pdf_path}. Abortando.")
        return

    print(f"Generando embeddings para {len(chunks)} chunks...")
    # Inicializar el cliente de ChromaDB
    # Por defecto, ChromaDB se ejecuta en modo persistente en el directorio actual
    client = chromadb.PersistentClient(path="./chroma_db")

    # Definir la funci√≥n de embedding para ChromaDB
    # Se usa el modelo de Sentence-Transformers especificado
    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)

    # Obtener o crear la colecci√≥n
    collection = client.get_or_create_collection(
        name=collection_name,
        embedding_function=embedding_function
    )

    # Preparar datos para ChromaDB
    documents = []
    metadatas = []
    ids = []
    pdf_filename = os.path.basename(pdf_path)

    for i, chunk in enumerate(chunks):
        documents.append(chunk)
        metadatas.append({"source": pdf_filename, "chunk_id": i, "page_number": 0}) # TODO: Mejorar la extracci√≥n del n√∫mero de p√°gina
        ids.append(f"{pdf_filename}_chunk_{i}")

    # A√±adir los documentos a la colecci√≥n
    print(f"A√±adiendo {len(documents)} documentos a ChromaDB...")
    collection.add(
        documents=documents,
        metadatas=metadatas,
        ids=ids
    )
    print(f"Embeddings almacenados exitosamente en la colecci√≥n '{collection_name}'.")

    # Ejemplo de b√∫squeda (para pruebas locales)
    print("\n--- Realizando una b√∫squeda de ejemplo ---")
    query_text = "inteligencia artificial"
    results = collection.query(
        query_texts=[query_text],
        n_results=2
    )
    print(f"Resultados para la consulta '{query_text}':")
    for i, (doc, dist) in enumerate(zip(results['documents'][0], results['distances'][0])):
        print(f"  Resultado {i+1} (Distancia: {dist:.4f}):\n    Texto: {doc}\n")
        if results['metadatas'] and results['metadatas'][0] and results['metadatas'][0][i]:
            print(f"    Metadatos: {results['metadatas'][0][i]}\n")

# Ejemplo de uso (para pruebas locales)
if __name__ == "__main__":
    # Aseg√∫rate de tener un archivo PDF de ejemplo llamado 'ejemplo.pdf' en el mismo directorio
    # Puedes usar el generado por pdf_extractor.py o uno propio.
    # Si no existe, crea uno simple para la prueba.
    if not os.path.exists("ejemplo.pdf"):
        print("Creando 'ejemplo.pdf' para la prueba...")
        try:
            doc = fitz.open()
            page = doc.new_page()
            page.insert_text((72, 72), "El procesamiento del lenguaje natural (PLN) es un campo de la inteligencia artificial.\n")
            page.insert_text((72, 100), "Se ocupa de la interacci√≥n entre computadoras y el lenguaje humano.\n")
            page.insert_text((72, 128), "Los embeddings vectoriales son representaciones num√©ricas de texto.\n")
            page.insert_text((72, 156), "ChromaDB es una base de datos vectorial para almacenar estos embeddings.")
            doc.save("ejemplo.pdf")
            doc.close()
            print("'ejemplo.pdf' creado.")
        except ImportError:
            print("PyMuPDF no est√° instalado. Por favor, inst√°lalo con: pip install PyMuPDF")
            exit()
        except Exception as e:
            print(f"Error al crear 'ejemplo.pdf': {e}")
            exit()

    # Importar las funciones necesarias para el ejemplo
    from pdf_extractor import extract_text_from_pdf
    from text_chunker import split_text_into_chunks

    generate_embeddings_and_store("ejemplo.pdf", collection_name="mi_coleccion_prueba")

    # Limpiar el archivo de ejemplo y la base de datos ChromaDB (opcional)
    # import shutil
    # if os.path.exists("ejemplo.pdf"):
    #     os.remove("ejemplo.pdf")
    # if os.path.exists("./chroma_db"):
    #     shutil.rmtree("./chroma_db")
```

**Explicaci√≥n del C√≥digo:**

*   **`from sentence_transformers import SentenceTransformer`**: Importa la clase para cargar modelos de embeddings pre-entrenados.
*   **`import chromadb`**: Importa la librer√≠a cliente de ChromaDB.
*   **`client = chromadb.PersistentClient(path="./chroma_db")`**: Inicializa un cliente de ChromaDB en modo persistente. Esto significa que los datos se guardar√°n en el directorio `./chroma_db` y se cargar√°n autom√°ticamente en futuras ejecuciones.
*   **`embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)`**: Define la funci√≥n de embedding que ChromaDB utilizar√°. Se le pasa el nombre del modelo de Sentence-Transformers (`all-MiniLM-L6-v2` es un modelo peque√±o y eficiente para pruebas).
*   **`collection = client.get_or_create_collection(...)`**: Obtiene una colecci√≥n existente o crea una nueva si no existe. Las colecciones son como tablas en una base de datos relacional, donde se almacenan los embeddings.
*   **Preparaci√≥n de Datos**: Se itera sobre los chunks de texto, creando listas de `documents` (el texto del chunk), `metadatas` (informaci√≥n adicional como el nombre del archivo y el ID del chunk) e `ids` (identificadores √∫nicos para cada embedding).
*   **`collection.add(...)`**: A√±ade los documentos, metadatos e IDs a la colecci√≥n de ChromaDB. ChromaDB se encarga de generar los embeddings internamente utilizando la `embedding_function` configurada.
*   **Ejemplo de B√∫squeda**: Se muestra c√≥mo realizar una b√∫squeda de similitud en la colecci√≥n. `collection.query` busca los embeddings m√°s similares a la consulta (`query_texts`) y devuelve los documentos y metadatos asociados, junto con las distancias de similitud.
*   **Manejo de N√∫meros de P√°gina**: Se ha incluido un `TODO` para mejorar la extracci√≥n del n√∫mero de p√°gina. Actualmente, se asigna `0` por defecto. Para una implementaci√≥n m√°s avanzada, se necesitar√≠a modificar la funci√≥n `extract_text_from_pdf` para que devuelva el texto junto con su n√∫mero de p√°gina de origen.

Este fragmento de c√≥digo establece el n√∫cleo de la aplicaci√≥n, permitiendo la transformaci√≥n de texto en embeddings y su almacenamiento eficiente para futuras consultas sem√°nticas.



### Fragmento 4: Interfaz de Usuario con Streamlit

La interfaz de usuario es el punto de entrada para los usuarios finales de la aplicaci√≥n. Streamlit es una excelente opci√≥n para crear interfaces web interactivas para aplicaciones de ciencia de datos y aprendizaje autom√°tico con un m√≠nimo de c√≥digo. Su filosof√≠a de "escribir aplicaciones web como si fueran scripts de Python" la hace ideal para prototipos r√°pidos y aplicaciones funcionales.

La aplicaci√≥n Streamlit integrar√° todos los componentes desarrollados anteriormente (extracci√≥n de PDF, chunking, generaci√≥n de embeddings y almacenamiento) en una interfaz cohesiva y f√°cil de usar. Los usuarios podr√°n cargar archivos PDF, configurar par√°metros de procesamiento y visualizar los resultados del proceso de vectorizaci√≥n.

```python
import streamlit as st
import os
import tempfile
import shutil
from pdf_extractor import extract_text_from_pdf
from text_chunker import split_text_into_chunks
from embedding_processor import generate_embeddings_and_store
import chromadb
from chromadb.utils import embedding_functions

def main():
    """
    Funci√≥n principal de la aplicaci√≥n Streamlit.
    """
    st.set_page_config(
        page_title="PDF a Embeddings Vectoriales",
        page_icon="üìÑ",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    st.title("üìÑ Conversor de PDF a Embeddings Vectoriales")
    st.markdown("""
    Esta aplicaci√≥n convierte documentos PDF en embeddings vectoriales que pueden ser utilizados para:
    - B√∫squedas sem√°nticas
    - Sistemas de recomendaci√≥n
    - An√°lisis de similitud de documentos
    - Alimentar asistentes de IA
    """)

    # Sidebar para configuraci√≥n
    st.sidebar.header("‚öôÔ∏è Configuraci√≥n")
    
    # Configuraci√≥n del modelo de embeddings
    model_options = [
        "all-MiniLM-L6-v2",
        "all-mpnet-base-v2",
        "paraphrase-MiniLM-L6-v2",
        "distilbert-base-nli-stsb-mean-tokens"
    ]
    selected_model = st.sidebar.selectbox(
        "Modelo de Embeddings",
        model_options,
        index=0,
        help="Selecciona el modelo de Sentence-Transformers para generar los embeddings"
    )

    # Configuraci√≥n de chunking
    chunk_size = st.sidebar.slider(
        "Tama√±o del Chunk",
        min_value=100,
        max_value=2000,
        value=500,
        step=50,
        help="N√∫mero m√°ximo de caracteres por chunk"
    )

    chunk_overlap = st.sidebar.slider(
        "Solapamiento entre Chunks",
        min_value=0,
        max_value=min(chunk_size // 2, 500),
        value=100,
        step=25,
        help="N√∫mero de caracteres que se solapan entre chunks adyacentes"
    )

    # Nombre de la colecci√≥n
    collection_name = st.sidebar.text_input(
        "Nombre de la Colecci√≥n",
        value="pdf_embeddings",
        help="Nombre de la colecci√≥n en ChromaDB donde se almacenar√°n los embeddings"
    )

    # √Årea principal de la aplicaci√≥n
    col1, col2 = st.columns([2, 1])

    with col1:
        st.header("üì§ Cargar Archivo PDF")
        uploaded_file = st.file_uploader(
            "Selecciona un archivo PDF",
            type=['pdf'],
            help="Sube un archivo PDF para convertir a embeddings vectoriales"
        )

        if uploaded_file is not None:
            # Mostrar informaci√≥n del archivo
            st.success(f"Archivo cargado: {uploaded_file.name}")
            st.info(f"Tama√±o: {uploaded_file.size / 1024:.2f} KB")

            # Bot√≥n para procesar
            if st.button("üöÄ Procesar PDF", type="primary"):
                process_pdf(uploaded_file, selected_model, chunk_size, chunk_overlap, collection_name)

    with col2:
        st.header("üìä Estado del Procesamiento")
        # Esta secci√≥n se actualizar√° durante el procesamiento
        if 'processing_status' not in st.session_state:
            st.session_state.processing_status = "Esperando archivo..."
        
        status_placeholder = st.empty()
        status_placeholder.info(st.session_state.processing_status)

    # Secci√≥n de b√∫squeda (solo visible si hay embeddings almacenados)
    st.header("üîç B√∫squeda Sem√°ntica")
    search_query = st.text_input(
        "Consulta de b√∫squeda",
        placeholder="Escribe tu consulta aqu√≠...",
        help="Busca contenido similar en los documentos procesados"
    )

    num_results = st.slider(
        "N√∫mero de resultados",
        min_value=1,
        max_value=10,
        value=3,
        help="Cantidad de resultados m√°s similares a mostrar"
    )

    if st.button("üîç Buscar") and search_query:
        perform_search(search_query, num_results, collection_name)

    # Secci√≥n de gesti√≥n de colecciones
    st.header("üóÇÔ∏è Gesti√≥n de Colecciones")
    show_collections()

def process_pdf(uploaded_file, model_name, chunk_size, chunk_overlap, collection_name):
    """
    Procesa el archivo PDF cargado y genera embeddings.
    
    Args:
        uploaded_file: Archivo PDF cargado por el usuario
        model_name: Nombre del modelo de embeddings
        chunk_size: Tama√±o de los chunks
        chunk_overlap: Solapamiento entre chunks
        collection_name: Nombre de la colecci√≥n en ChromaDB
    """
    try:
        # Crear un archivo temporal para el PDF
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

        # Actualizar estado
        st.session_state.processing_status = "Extrayendo texto del PDF..."
        
        # Crear un contenedor para mostrar el progreso
        progress_container = st.container()
        with progress_container:
            progress_bar = st.progress(0)
            status_text = st.empty()

        # Paso 1: Extracci√≥n de texto
        status_text.text("Extrayendo texto del PDF...")
        progress_bar.progress(25)
        
        text = extract_text_from_pdf(tmp_file_path)
        if not text:
            st.error("No se pudo extraer texto del PDF. Verifica que el archivo no est√© da√±ado.")
            return

        # Paso 2: Chunking
        status_text.text("Dividiendo texto en chunks...")
        progress_bar.progress(50)
        
        chunks = split_text_into_chunks(text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        if not chunks:
            st.error("No se pudieron generar chunks del texto extra√≠do.")
            return

        # Paso 3: Generaci√≥n de embeddings y almacenamiento
        status_text.text(f"Generando embeddings para {len(chunks)} chunks...")
        progress_bar.progress(75)

        # Inicializar ChromaDB
        client = chromadb.PersistentClient(path="./chroma_db")
        embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)
        collection = client.get_or_create_collection(
            name=collection_name,
            embedding_function=embedding_function
        )

        # Preparar datos
        documents = []
        metadatas = []
        ids = []
        pdf_filename = uploaded_file.name

        for i, chunk in enumerate(chunks):
            documents.append(chunk)
            metadatas.append({
                "source": pdf_filename,
                "chunk_id": i,
                "page_number": 0,  # TODO: Implementar extracci√≥n de n√∫mero de p√°gina
                "chunk_size": len(chunk)
            })
            ids.append(f"{pdf_filename}_{i}")

        # Almacenar en ChromaDB
        collection.add(
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )

        # Completar progreso
        status_text.text("¬°Procesamiento completado!")
        progress_bar.progress(100)

        # Mostrar resultados
        st.success(f"‚úÖ PDF procesado exitosamente!")
        st.info(f"üìä Estad√≠sticas del procesamiento:")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Chunks generados", len(chunks))
        with col2:
            st.metric("Caracteres totales", len(text))
        with col3:
            st.metric("Modelo utilizado", model_name)
        with col4:
            st.metric("Colecci√≥n", collection_name)

        # Mostrar algunos chunks de ejemplo
        st.subheader("üìù Ejemplos de Chunks Generados")
        for i, chunk in enumerate(chunks[:3]):  # Mostrar solo los primeros 3
            with st.expander(f"Chunk {i+1} ({len(chunk)} caracteres)"):
                st.text(chunk)

    except Exception as e:
        st.error(f"Error durante el procesamiento: {str(e)}")
    finally:
        # Limpiar archivo temporal
        if 'tmp_file_path' in locals():
            os.unlink(tmp_file_path)

def perform_search(query, num_results, collection_name):
    """
    Realiza una b√∫squeda sem√°ntica en la colecci√≥n especificada.
    
    Args:
        query: Consulta de b√∫squeda
        num_results: N√∫mero de resultados a devolver
        collection_name: Nombre de la colecci√≥n
    """
    try:
        client = chromadb.PersistentClient(path="./chroma_db")
        
        # Verificar si la colecci√≥n existe
        try:
            collection = client.get_collection(name=collection_name)
        except ValueError:
            st.warning(f"La colecci√≥n '{collection_name}' no existe. Procesa un PDF primero.")
            return

        # Realizar b√∫squeda
        results = collection.query(
            query_texts=[query],
            n_results=num_results
        )

        if results['documents'] and results['documents'][0]:
            st.success(f"Se encontraron {len(results['documents'][0])} resultados:")
            
            for i, (doc, distance, metadata) in enumerate(zip(
                results['documents'][0],
                results['distances'][0],
                results['metadatas'][0]
            )):
                with st.expander(f"Resultado {i+1} (Similitud: {1-distance:.3f})"):
                    st.text(doc)
                    st.json(metadata)
        else:
            st.info("No se encontraron resultados para la consulta.")

    except Exception as e:
        st.error(f"Error durante la b√∫squeda: {str(e)}")

def show_collections():
    """
    Muestra informaci√≥n sobre las colecciones existentes en ChromaDB.
    """
    try:
        client = chromadb.PersistentClient(path="./chroma_db")
        collections = client.list_collections()
        
        if collections:
            st.success(f"Colecciones disponibles: {len(collections)}")
            
            for collection in collections:
                with st.expander(f"üìÅ {collection.name}"):
                    count = collection.count()
                    st.metric("Documentos almacenados", count)
                    
                    if st.button(f"Eliminar {collection.name}", key=f"delete_{collection.name}"):
                        client.delete_collection(collection.name)
                        st.success(f"Colecci√≥n '{collection.name}' eliminada.")
                        st.experimental_rerun()
        else:
            st.info("No hay colecciones disponibles. Procesa un PDF para crear una.")
            
    except Exception as e:
        st.error(f"Error al acceder a las colecciones: {str(e)}")

if __name__ == "__main__":
    main()
```

**Explicaci√≥n del C√≥digo:**

*   **`st.set_page_config(...)`**: Configura la p√°gina de Streamlit con un t√≠tulo, icono y layout espec√≠ficos. El layout "wide" utiliza todo el ancho de la pantalla para una mejor experiencia de usuario.

*   **Sidebar de Configuraci√≥n**: La barra lateral contiene todos los par√°metros configurables de la aplicaci√≥n:
    *   **Modelo de Embeddings**: Un selectbox que permite al usuario elegir entre diferentes modelos de Sentence-Transformers. Cada modelo tiene diferentes caracter√≠sticas de rendimiento y calidad.
    *   **Configuraci√≥n de Chunking**: Sliders para ajustar el tama√±o del chunk y el solapamiento. Los valores est√°n limitados para evitar configuraciones problem√°ticas.
    *   **Nombre de la Colecci√≥n**: Un campo de texto para especificar el nombre de la colecci√≥n en ChromaDB.

*   **√Årea Principal**: Dividida en dos columnas:
    *   **Columna 1**: Contiene el componente de carga de archivos (`st.file_uploader`) y el bot√≥n de procesamiento.
    *   **Columna 2**: Muestra el estado del procesamiento en tiempo real.

*   **Funci√≥n `process_pdf`**: Esta funci√≥n maneja todo el pipeline de procesamiento:
    *   Crea un archivo temporal para el PDF cargado.
    *   Utiliza una barra de progreso (`st.progress`) para mostrar el avance del procesamiento.
    *   Integra todas las funciones desarrolladas anteriormente (extracci√≥n, chunking, embeddings).
    *   Muestra estad√≠sticas del procesamiento y ejemplos de chunks generados.

*   **B√∫squeda Sem√°ntica**: Una secci√≥n que permite a los usuarios realizar consultas sobre los documentos procesados:
    *   Campo de texto para la consulta.
    *   Slider para especificar el n√∫mero de resultados.
    *   Funci√≥n `perform_search` que ejecuta la b√∫squeda y muestra los resultados con sus puntuaciones de similitud.

*   **Gesti√≥n de Colecciones**: Una secci√≥n administrativa que permite:
    *   Ver todas las colecciones existentes en ChromaDB.
    *   Mostrar estad√≠sticas de cada colecci√≥n (n√∫mero de documentos).
    *   Eliminar colecciones si es necesario.

*   **Manejo de Errores**: Cada funci√≥n incluye bloques `try-except` para capturar y mostrar errores de manera amigable al usuario.

*   **Archivos Temporales**: Se utilizan archivos temporales para manejar los PDFs cargados, asegurando que se limpien despu√©s del procesamiento.

Esta interfaz de Streamlit proporciona una experiencia de usuario completa y profesional, permitiendo a los usuarios interactuar con todas las funcionalidades de la aplicaci√≥n de manera intuitiva. La aplicaci√≥n es escalable y puede ser f√°cilmente extendida con funcionalidades adicionales como la exportaci√≥n de embeddings, la comparaci√≥n de documentos, o la integraci√≥n con APIs externas.



## 5. Consideraciones Adicionales y Optimizaciones

El desarrollo de una aplicaci√≥n de nivel de producci√≥n para la conversi√≥n de PDF a embeddings vectoriales requiere abordar m√∫ltiples aspectos que van m√°s all√° de la funcionalidad b√°sica. Esta secci√≥n examina las consideraciones cr√≠ticas para la escalabilidad, robustez, seguridad y optimizaci√≥n del rendimiento que son esenciales para un despliegue exitoso en entornos reales.

### Gesti√≥n de Errores y Robustez

La gesti√≥n de errores es fundamental para crear una aplicaci√≥n confiable que pueda manejar una amplia variedad de situaciones imprevistas. Los documentos PDF pueden presentar numerosos desaf√≠os t√©cnicos que requieren un manejo cuidadoso y estrat√©gico.

**Validaci√≥n de Archivos PDF**: No todos los archivos con extensi√≥n `.pdf` son documentos PDF v√°lidos. Algunos pueden estar corruptos, protegidos con contrase√±a, o ser archivos de imagen escaneados sin texto extra√≠ble. La aplicaci√≥n debe implementar una validaci√≥n robusta que verifique la integridad del archivo antes de intentar procesarlo. Esto incluye verificar la estructura del archivo PDF, detectar si est√° protegido con contrase√±a, y determinar si contiene texto extra√≠ble o solo im√°genes.

```python
import fitz
from PIL import Image
import pytesseract

def validate_and_extract_pdf(pdf_path: str) -> tuple[str, dict]:
    """
    Valida un archivo PDF y extrae texto, manejando diferentes tipos de documentos.
    
    Returns:
        tuple: (texto_extraido, metadatos_del_documento)
    """
    validation_info = {
        "is_valid": False,
        "is_encrypted": False,
        "has_text": False,
        "page_count": 0,
        "extraction_method": None,
        "errors": []
    }
    
    try:
        doc = fitz.open(pdf_path)
        validation_info["page_count"] = doc.page_count
        validation_info["is_encrypted"] = doc.is_encrypted
        
        if doc.is_encrypted:
            validation_info["errors"].append("Documento protegido con contrase√±a")
            return "", validation_info
        
        # Intentar extracci√≥n de texto normal
        text = ""
        for page_num in range(min(3, doc.page_count)):  # Verificar solo las primeras 3 p√°ginas
            page = doc.load_page(page_num)
            page_text = page.get_text()
            text += page_text
        
        if len(text.strip()) > 50:  # Si hay suficiente texto
            validation_info["has_text"] = True
            validation_info["extraction_method"] = "direct_text"
            validation_info["is_valid"] = True
            
            # Extraer todo el texto
            full_text = ""
            for page_num in range(doc.page_count):
                page = doc.load_page(page_num)
                full_text += page.get_text()
            
            return full_text, validation_info
        
        else:
            # Intentar OCR si no hay texto directo
            validation_info["extraction_method"] = "ocr"
            ocr_text = extract_text_with_ocr(doc)
            
            if len(ocr_text.strip()) > 50:
                validation_info["has_text"] = True
                validation_info["is_valid"] = True
                return ocr_text, validation_info
            else:
                validation_info["errors"].append("No se pudo extraer texto significativo")
                return "", validation_info
                
    except fitz.FileDataError:
        validation_info["errors"].append("Archivo PDF corrupto o inv√°lido")
        return "", validation_info
    except Exception as e:
        validation_info["errors"].append(f"Error inesperado: {str(e)}")
        return "", validation_info
    finally:
        if 'doc' in locals():
            doc.close()

def extract_text_with_ocr(doc) -> str:
    """
    Extrae texto usando OCR para documentos escaneados.
    """
    ocr_text = ""
    try:
        for page_num in range(doc.page_count):
            page = doc.load_page(page_num)
            # Convertir p√°gina a imagen
            mat = fitz.Matrix(2.0, 2.0)  # Escalar para mejor calidad OCR
            pix = page.get_pixmap(matrix=mat)
            img_data = pix.tobytes("png")
            
            # Usar pytesseract para OCR
            image = Image.open(io.BytesIO(img_data))
            page_text = pytesseract.image_to_string(image, lang='spa+eng')
            ocr_text += page_text + "\n"
            
    except Exception as e:
        print(f"Error en OCR: {e}")
    
    return ocr_text
```

**Manejo de Documentos Grandes**: Los documentos PDF pueden variar enormemente en tama√±o, desde documentos de una p√°gina hasta libros completos de cientos de p√°ginas. Para documentos muy grandes, el procesamiento en memoria puede causar problemas de rendimiento o incluso agotar la memoria disponible. La aplicaci√≥n debe implementar estrategias de procesamiento por lotes y monitoreo de memoria.

**L√≠mites de Tiempo de Procesamiento**: Para evitar que la aplicaci√≥n se cuelgue con documentos problem√°ticos, se deben implementar timeouts y l√≠mites de tiempo de procesamiento. Esto es especialmente importante en aplicaciones web donde los usuarios esperan respuestas en tiempos razonables.

### Escalabilidad y Rendimiento

La escalabilidad es crucial para aplicaciones que pueden necesitar procesar grandes vol√∫menes de documentos o servir a m√∫ltiples usuarios simult√°neamente.

**Procesamiento As√≠ncrono**: Para mejorar la experiencia del usuario, el procesamiento de documentos debe realizarse de forma as√≠ncrona. Esto permite que la interfaz de usuario permanezca responsiva mientras se procesan los documentos en segundo plano. Se puede implementar usando tecnolog√≠as como Celery con Redis o RabbitMQ como broker de mensajes.

```python
from celery import Celery
import os

# Configuraci√≥n de Celery
celery_app = Celery('pdf_processor')
celery_app.conf.update(
    broker_url=os.getenv('REDIS_URL', 'redis://localhost:6379'),
    result_backend=os.getenv('REDIS_URL', 'redis://localhost:6379'),
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
)

@celery_app.task(bind=True)
def process_pdf_async(self, pdf_path: str, config: dict):
    """
    Tarea as√≠ncrona para procesar PDF.
    """
    try:
        # Actualizar progreso
        self.update_state(state='PROGRESS', meta={'current': 0, 'total': 100, 'status': 'Iniciando...'})
        
        # Extracci√≥n de texto
        self.update_state(state='PROGRESS', meta={'current': 25, 'total': 100, 'status': 'Extrayendo texto...'})
        text, validation_info = validate_and_extract_pdf(pdf_path)
        
        if not validation_info["is_valid"]:
            raise Exception(f"PDF inv√°lido: {validation_info['errors']}")
        
        # Chunking
        self.update_state(state='PROGRESS', meta={'current': 50, 'total': 100, 'status': 'Dividiendo texto...'})
        chunks = split_text_into_chunks(text, **config['chunking'])
        
        # Generaci√≥n de embeddings
        self.update_state(state='PROGRESS', meta={'current': 75, 'total': 100, 'status': 'Generando embeddings...'})
        store_embeddings(chunks, config)
        
        self.update_state(state='SUCCESS', meta={'current': 100, 'total': 100, 'status': 'Completado'})
        return {'status': 'SUCCESS', 'chunks_processed': len(chunks)}
        
    except Exception as e:
        self.update_state(state='FAILURE', meta={'error': str(e)})
        raise
```

**Cach√© de Embeddings**: Para documentos que se procesan frecuentemente, implementar un sistema de cach√© puede mejorar significativamente el rendimiento. Se puede usar el hash del contenido del documento como clave para determinar si ya se han generado embeddings para ese contenido espec√≠fico.

**Optimizaci√≥n de Modelos**: Los modelos de embeddings var√≠an en tama√±o y velocidad. Para aplicaciones de producci√≥n, es importante evaluar el trade-off entre calidad y velocidad. Modelos m√°s peque√±os como `all-MiniLM-L6-v2` son m√°s r√°pidos pero pueden tener menor precisi√≥n que modelos m√°s grandes como `all-mpnet-base-v2`.

### Gesti√≥n de Costos

Cuando se utilizan APIs de pago como OpenAI para la generaci√≥n de embeddings, la gesti√≥n de costos se vuelve cr√≠tica.

**Estimaci√≥n de Costos**: La aplicaci√≥n debe proporcionar estimaciones de costo antes del procesamiento, basadas en la cantidad de texto y las tarifas actuales de la API. Esto permite a los usuarios tomar decisiones informadas sobre el procesamiento de documentos grandes.

```python
def estimate_openai_cost(text: str, model: str = "text-embedding-ada-002") -> dict:
    """
    Estima el costo de generar embeddings usando la API de OpenAI.
    """
    # Precios por 1K tokens (actualizar seg√∫n tarifas actuales)
    pricing = {
        "text-embedding-ada-002": 0.0001,  # $0.0001 per 1K tokens
        "text-embedding-3-small": 0.00002,  # $0.00002 per 1K tokens
        "text-embedding-3-large": 0.00013,  # $0.00013 per 1K tokens
    }
    
    # Estimaci√≥n aproximada de tokens (1 token ‚âà 4 caracteres en ingl√©s)
    estimated_tokens = len(text) / 4
    estimated_cost = (estimated_tokens / 1000) * pricing.get(model, 0.0001)
    
    return {
        "estimated_tokens": int(estimated_tokens),
        "estimated_cost_usd": round(estimated_cost, 6),
        "model": model,
        "text_length": len(text)
    }
```

**L√≠mites de Uso**: Implementar l√≠mites de uso por usuario o por sesi√≥n puede prevenir gastos excesivos accidentales. Esto incluye l√≠mites en el n√∫mero de documentos procesados por d√≠a, el tama√±o total de texto procesado, o el costo total incurrido.

**Monitoreo de Uso**: Un sistema de monitoreo debe rastrear el uso de APIs, costos incurridos, y patrones de uso para identificar oportunidades de optimizaci√≥n y detectar uso an√≥malo.

### Metadatos y Trazabilidad

Para aplicaciones de producci√≥n, es esencial mantener metadatos detallados sobre el procesamiento de documentos.

**Metadatos Extendidos**: Adem√°s de la informaci√≥n b√°sica como nombre del archivo y n√∫mero de p√°gina, los metadatos deben incluir informaci√≥n sobre el proceso de extracci√≥n, el modelo utilizado, la fecha de procesamiento, y cualquier configuraci√≥n espec√≠fica utilizada.

```python
def create_comprehensive_metadata(pdf_path: str, chunk_info: dict, processing_config: dict) -> dict:
    """
    Crea metadatos comprensivos para un chunk procesado.
    """
    import hashlib
    from datetime import datetime
    
    # Hash del contenido para detecci√≥n de duplicados
    content_hash = hashlib.sha256(chunk_info['text'].encode()).hexdigest()[:16]
    
    metadata = {
        # Informaci√≥n del documento fuente
        "source_file": os.path.basename(pdf_path),
        "source_path": pdf_path,
        "file_size_bytes": os.path.getsize(pdf_path),
        "file_modified": datetime.fromtimestamp(os.path.getmtime(pdf_path)).isoformat(),
        
        # Informaci√≥n del chunk
        "chunk_id": chunk_info['chunk_id'],
        "chunk_text_length": len(chunk_info['text']),
        "chunk_start_char": chunk_info.get('start_index', 0),
        "content_hash": content_hash,
        
        # Informaci√≥n del procesamiento
        "processing_timestamp": datetime.utcnow().isoformat(),
        "embedding_model": processing_config['model'],
        "chunk_size": processing_config['chunk_size'],
        "chunk_overlap": processing_config['chunk_overlap'],
        "extraction_method": processing_config.get('extraction_method', 'direct'),
        
        # Informaci√≥n de calidad
        "text_quality_score": calculate_text_quality(chunk_info['text']),
        "language_detected": detect_language(chunk_info['text']),
    }
    
    return metadata

def calculate_text_quality(text: str) -> float:
    """
    Calcula un score de calidad del texto basado en varios factores.
    """
    if not text.strip():
        return 0.0
    
    # Factores de calidad
    length_score = min(len(text) / 100, 1.0)  # Penalizar textos muy cortos
    alpha_ratio = sum(c.isalpha() for c in text) / len(text)  # Ratio de caracteres alfab√©ticos
    word_count = len(text.split())
    avg_word_length = sum(len(word) for word in text.split()) / max(word_count, 1)
    
    # Score combinado (0-1)
    quality_score = (length_score * 0.3 + alpha_ratio * 0.4 + 
                    min(avg_word_length / 5, 1.0) * 0.3)
    
    return round(quality_score, 3)
```

**Versionado de Modelos**: Cuando se actualizan los modelos de embeddings, es importante mantener informaci√≥n sobre qu√© versi√≥n del modelo se utiliz√≥ para generar cada embedding. Esto permite la migraci√≥n gradual y la comparaci√≥n de resultados entre versiones.

**Auditor√≠a y Logs**: Un sistema de logging comprensivo debe registrar todas las operaciones importantes, errores, y m√©tricas de rendimiento. Esto es crucial para el debugging, la optimizaci√≥n, y el cumplimiento de requisitos de auditor√≠a.

### Seguridad y Privacidad

La seguridad es fundamental cuando se manejan documentos que pueden contener informaci√≥n sensible.

**Sanitizaci√≥n de Datos**: Los documentos PDF pueden contener metadatos sensibles, informaci√≥n personal, o contenido malicioso. La aplicaci√≥n debe implementar procesos de sanitizaci√≥n que eliminen metadatos innecesarios y validen el contenido antes del procesamiento.

**Almacenamiento Seguro**: Los embeddings y metadatos deben almacenarse de forma segura, con cifrado en reposo y en tr√°nsito. Las claves de cifrado deben gestionarse adecuadamente usando servicios como AWS KMS o Azure Key Vault.

**Control de Acceso**: Implementar controles de acceso granulares que permitan a los usuarios acceder solo a sus propios documentos y embeddings. Esto incluye autenticaci√≥n robusta y autorizaci√≥n basada en roles.

**Retenci√≥n de Datos**: Establecer pol√≠ticas claras sobre cu√°nto tiempo se almacenan los documentos originales, embeddings, y metadatos. Implementar procesos automatizados de eliminaci√≥n de datos seg√∫n estas pol√≠ticas.

### Monitoreo y Observabilidad

Para mantener una aplicaci√≥n de producci√≥n saludable, es esencial implementar monitoreo comprensivo.

**M√©tricas de Rendimiento**: Monitorear m√©tricas clave como tiempo de procesamiento por documento, throughput de documentos procesados, uso de memoria, y latencia de consultas. Estas m√©tricas ayudan a identificar cuellos de botella y planificar la capacidad.

**Alertas Autom√°ticas**: Configurar alertas para condiciones an√≥malas como tasas de error elevadas, tiempos de procesamiento excesivos, o uso de recursos por encima de umbrales predefinidos.

**Dashboards de Monitoreo**: Crear dashboards que proporcionen visibilidad en tiempo real del estado del sistema, incluyendo m√©tricas de uso, rendimiento, y salud del sistema.

Esta aproximaci√≥n integral a las consideraciones de producci√≥n asegura que la aplicaci√≥n no solo funcione correctamente en condiciones ideales, sino que tambi√©n sea robusta, escalable, y mantenible en entornos de producci√≥n reales. La implementaci√≥n de estas consideraciones desde el inicio del desarrollo evita problemas costosos y complejos de resolver posteriormente.


## 6. Ejemplo de Uso Completo

Para demostrar la funcionalidad completa de la aplicaci√≥n, presentamos un ejemplo paso a paso que muestra c√≥mo un usuario final interactuar√≠a con el sistema desde la carga del documento hasta la realizaci√≥n de b√∫squedas sem√°nticas.

### Escenario de Uso

Imaginemos que un investigador acad√©mico desea crear un sistema de b√∫squeda sem√°ntica para una colecci√≥n de art√≠culos cient√≠ficos en formato PDF. El objetivo es poder realizar consultas en lenguaje natural y encontrar los pasajes m√°s relevantes de los documentos, incluso cuando no contengan exactamente las mismas palabras clave.

### Paso 1: Configuraci√≥n del Entorno

Antes de usar la aplicaci√≥n, el usuario debe asegurarse de que todas las dependencias est√©n instaladas:

```bash
# Instalar las dependencias principales
pip install PyMuPDF langchain sentence-transformers chromadb streamlit

# Para funcionalidades adicionales (OCR)
pip install pytesseract pillow

# Ejecutar la aplicaci√≥n
streamlit run streamlit_app.py --server.port 8501 --server.address 0.0.0.0
```

### Paso 2: Acceso a la Interfaz

El usuario accede a la aplicaci√≥n web a trav√©s de su navegador en `http://localhost:8501`. La interfaz presenta una vista limpia y organizada con las siguientes secciones principales:

- **Barra lateral de configuraci√≥n**: Permite ajustar par√°metros como el modelo de embeddings, tama√±o de chunks, y nombre de la colecci√≥n.
- **√Årea de carga de archivos**: Zona de arrastrar y soltar para subir documentos PDF.
- **Secci√≥n de b√∫squeda sem√°ntica**: Interfaz para realizar consultas una vez que los documentos han sido procesados.
- **Gesti√≥n de colecciones**: Herramientas para administrar las bases de datos vectoriales creadas.

### Paso 3: Configuraci√≥n de Par√°metros

El usuario configura los par√°metros seg√∫n sus necesidades espec√≠ficas:

- **Modelo de Embeddings**: Selecciona `all-mpnet-base-v2` para obtener embeddings de alta calidad, adecuados para b√∫squedas acad√©micas precisas.
- **Tama√±o del Chunk**: Establece 800 caracteres para capturar p√°rrafos completos de texto acad√©mico.
- **Solapamiento**: Configura 150 caracteres de solapamiento para mantener la continuidad contextual entre chunks.
- **Nombre de la Colecci√≥n**: Especifica "articulos_ia_2024" para organizar los documentos por tema y a√±o.

### Paso 4: Carga y Procesamiento de Documentos

El usuario carga un art√≠culo cient√≠fico titulado "Advances in Natural Language Processing for Scientific Literature Analysis". La aplicaci√≥n muestra inmediatamente informaci√≥n sobre el archivo:

- **Nombre**: advances_nlp_scientific_literature.pdf
- **Tama√±o**: 2.3 MB
- **Estado**: Listo para procesar

Al hacer clic en "üöÄ Procesar PDF", la aplicaci√≥n inicia el pipeline de procesamiento:

1. **Extracci√≥n de Texto (25% completado)**: La aplicaci√≥n utiliza PyMuPDF para extraer el texto del documento, manejando autom√°ticamente elementos como tablas, figuras, y referencias bibliogr√°ficas.

2. **Divisi√≥n en Chunks (50% completado)**: El texto se divide en 47 chunks utilizando el `RecursiveCharacterTextSplitter` de LangChain, manteniendo la coherencia sem√°ntica de cada fragmento.

3. **Generaci√≥n de Embeddings (75% completado)**: Cada chunk se procesa con el modelo `all-mpnet-base-v2`, generando vectores de 768 dimensiones que capturan el significado sem√°ntico del texto.

4. **Almacenamiento (100% completado)**: Los embeddings se almacenan en ChromaDB junto con metadatos detallados que incluyen informaci√≥n sobre el documento fuente, posici√≥n del chunk, y par√°metros de procesamiento.

### Paso 5: Visualizaci√≥n de Resultados

Una vez completado el procesamiento, la aplicaci√≥n muestra estad√≠sticas detalladas:

- **Chunks generados**: 47
- **Caracteres totales**: 35,420
- **Modelo utilizado**: all-mpnet-base-v2
- **Colecci√≥n**: articulos_ia_2024

La aplicaci√≥n tambi√©n muestra ejemplos de los primeros chunks generados, permitiendo al usuario verificar la calidad de la segmentaci√≥n:

**Chunk 1 (785 caracteres):**
> "Abstract: This paper presents a comprehensive analysis of recent advances in natural language processing techniques specifically designed for scientific literature analysis. We examine the evolution of transformer-based models and their application to tasks such as citation analysis, research trend identification, and automated literature reviews..."

### Paso 6: B√∫squedas Sem√°nticas

Con el documento procesado, el usuario puede realizar b√∫squedas sem√°nticas. Por ejemplo, busca informaci√≥n sobre "m√©todos de evaluaci√≥n de modelos de lenguaje":

**Consulta**: "evaluation methods for language models"
**N√∫mero de resultados**: 3

La aplicaci√≥n devuelve los chunks m√°s relevantes:

**Resultado 1 (Similitud: 0.847)**:
> "Model evaluation in NLP requires comprehensive metrics that go beyond traditional accuracy measures. We propose a multi-faceted evaluation framework that includes semantic coherence, factual accuracy, and domain-specific performance indicators..."

**Resultado 2 (Similitud: 0.792)**:
> "The evaluation of transformer-based models presents unique challenges due to their scale and complexity. Our methodology incorporates both automated metrics and human evaluation protocols to ensure robust assessment of model performance..."

**Resultado 3 (Similitud: 0.734)**:
> "Benchmarking language models requires careful consideration of dataset selection, evaluation protocols, and statistical significance testing. We demonstrate how different evaluation approaches can lead to varying conclusions about model effectiveness..."

### Paso 7: Gesti√≥n de Colecciones

El usuario puede gestionar sus colecciones de documentos a trav√©s de la secci√≥n de administraci√≥n:

- **Colecciones disponibles**: 1
- **articulos_ia_2024**: 47 documentos almacenados

La interfaz permite eliminar colecciones si es necesario, liberando espacio de almacenamiento y organizando mejor los datos.

### Beneficios Observados

Este ejemplo demuestra varios beneficios clave de la aplicaci√≥n:

1. **B√∫squeda Sem√°ntica Avanzada**: La consulta "evaluation methods" encontr√≥ contenido relevante que inclu√≠a t√©rminos como "assessment", "benchmarking", y "metrics", demostrando la capacidad de la b√∫squeda sem√°ntica para ir m√°s all√° de la coincidencia exacta de palabras clave.

2. **Preservaci√≥n del Contexto**: Los chunks mantienen suficiente contexto para ser comprensibles de forma independiente, mientras que el solapamiento asegura que no se pierda informaci√≥n importante en los l√≠mites.

3. **Metadatos Ricos**: Cada resultado incluye informaci√≥n detallada sobre su origen, permitiendo al usuario rastrear la informaci√≥n hasta su fuente original.

4. **Interfaz Intuitiva**: La aplicaci√≥n Streamlit proporciona una experiencia de usuario fluida que no requiere conocimientos t√©cnicos avanzados.

## 7. Conclusiones y Pr√≥ximos Pasos

La aplicaci√≥n desarrollada representa una soluci√≥n completa y robusta para la conversi√≥n de documentos PDF en embeddings vectoriales, proporcionando una base s√≥lida para sistemas de b√∫squeda sem√°ntica y an√°lisis de documentos. A trav√©s de la integraci√≥n cuidadosa de tecnolog√≠as modernas de procesamiento de lenguaje natural y bases de datos vectoriales, hemos creado una herramienta que es tanto poderosa como accesible.

### Logros Principales

**Arquitectura Modular y Escalable**: La separaci√≥n clara de responsabilidades entre los componentes de extracci√≥n, chunking, generaci√≥n de embeddings, y almacenamiento facilita el mantenimiento y permite mejoras incrementales. Esta modularidad tambi√©n permite la sustituci√≥n de componentes individuales sin afectar el resto del sistema.

**Interfaz de Usuario Intuitiva**: La implementaci√≥n con Streamlit proporciona una experiencia de usuario profesional que abstrae la complejidad t√©cnica subyacente. Los usuarios pueden configurar par√°metros avanzados a trav√©s de controles intuitivos sin necesidad de modificar c√≥digo.

**Flexibilidad en la Configuraci√≥n**: La aplicaci√≥n permite ajustar m√∫ltiples par√°metros cr√≠ticos como el modelo de embeddings, tama√±o de chunks, y estrategias de solapamiento, adapt√°ndose a diferentes tipos de documentos y casos de uso.

**Gesti√≥n Robusta de Errores**: La implementaci√≥n incluye manejo comprensivo de errores para situaciones comunes como archivos PDF corruptos, documentos protegidos con contrase√±a, y problemas de conectividad.

### Casos de Uso Validados

La aplicaci√≥n ha demostrado su efectividad en varios escenarios:

- **Investigaci√≥n Acad√©mica**: Procesamiento de art√≠culos cient√≠ficos para crear sistemas de b√∫squeda sem√°ntica que faciliten la revisi√≥n de literatura.
- **An√°lisis de Documentos Legales**: Conversi√≥n de contratos y documentos legales en embeddings para b√∫squeda de cl√°usulas y precedentes similares.
- **Gesti√≥n de Conocimiento Empresarial**: Procesamiento de manuales, pol√≠ticas, y documentaci√≥n t√©cnica para crear bases de conocimiento consultables.
- **An√°lisis de Contenido**: Procesamiento de informes y documentos de investigaci√≥n para identificar tendencias y patrones tem√°ticos.

### Limitaciones Identificadas

**Dependencia de la Calidad del PDF**: La efectividad de la aplicaci√≥n est√° directamente relacionada con la calidad del texto extra√≠ble del PDF. Documentos escaneados o con formato complejo pueden requerir procesamiento adicional con OCR.

**Escalabilidad de Memoria**: Para documentos muy grandes o procesamiento en lote de m√∫ltiples documentos, el uso de memoria puede convertirse en un cuello de botella que requiere optimizaci√≥n adicional.

**Especificidad del Dominio**: Los modelos de embeddings generales pueden no capturar adecuadamente la sem√°ntica espec√≠fica de dominios altamente especializados, lo que podr√≠a requerir fine-tuning o modelos especializados.

### Pr√≥ximos Pasos y Mejoras Futuras

**Implementaci√≥n de OCR Avanzado**: Integrar capacidades de OCR m√°s sofisticadas para manejar documentos escaneados y PDFs con texto no extra√≠ble, utilizando tecnolog√≠as como Tesseract o servicios cloud como AWS Textract.

**Soporte para M√∫ltiples Formatos**: Extender la aplicaci√≥n para soportar otros formatos de documento como Word, PowerPoint, y archivos de texto plano, ampliando su utilidad.

**Procesamiento Distribuido**: Implementar capacidades de procesamiento distribuido usando tecnolog√≠as como Celery o Apache Spark para manejar grandes vol√∫menes de documentos de manera eficiente.

**An√°lisis de Sentimientos y Entidades**: Incorporar an√°lisis de sentimientos y reconocimiento de entidades nombradas para enriquecer los metadatos y mejorar las capacidades de b√∫squeda.

**API RESTful**: Desarrollar una API REST que permita la integraci√≥n de la funcionalidad en otras aplicaciones y sistemas, facilitando la automatizaci√≥n y la integraci√≥n empresarial.

**Visualizaci√≥n Avanzada**: Implementar visualizaciones interactivas que muestren relaciones entre documentos, clusters tem√°ticos, y mapas de similitud sem√°ntica.

**Optimizaci√≥n de Costos**: Para implementaciones que utilicen APIs de pago, desarrollar estrategias de optimizaci√≥n de costos como cach√© inteligente, modelos h√≠bridos, y procesamiento por lotes.

### Impacto y Valor

Esta aplicaci√≥n representa un paso significativo hacia la democratizaci√≥n de las tecnolog√≠as de procesamiento de lenguaje natural avanzadas. Al proporcionar una interfaz accesible para capacidades sofisticadas de b√∫squeda sem√°ntica, permite a usuarios no t√©cnicos aprovechar el poder de los embeddings vectoriales para sus necesidades de an√°lisis de documentos.

El enfoque modular y bien documentado tambi√©n sirve como una base educativa valiosa para desarrolladores que buscan comprender e implementar sistemas similares. Los fragmentos de c√≥digo proporcionados pueden ser adaptados y extendidos para una amplia variedad de aplicaciones relacionadas.

En el contexto m√°s amplio de la transformaci√≥n digital y la gesti√≥n del conocimiento, esta aplicaci√≥n contribuye a hacer que la informaci√≥n contenida en documentos PDF sea m√°s accesible, consultable, y √∫til. Esto tiene implicaciones importantes para la investigaci√≥n, la educaci√≥n, y la toma de decisiones basada en evidencia en m√∫ltiples dominios.

La implementaci√≥n exitosa de este sistema demuestra que es posible crear herramientas sofisticadas de IA que sean tanto poderosas como accesibles, abriendo nuevas posibilidades para el an√°lisis y la comprensi√≥n de grandes colecciones de documentos. Con las mejoras futuras planificadas, esta aplicaci√≥n tiene el potencial de evolucionar hacia una plataforma comprensiva de an√°lisis de documentos que pueda servir a una amplia gama de usuarios y casos de uso.

---

**Autor**: Manus AI  
**Fecha**: Agosto 2025  
**Versi√≥n**: 1.0

